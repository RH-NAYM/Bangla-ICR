{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import pprint\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    OCR DATASET CLASS\n",
    "    Dataset Used = BanglaWriting\n",
    "    Dataset Manual = https://arxiv.org/pdf/2011.07499.pdf\n",
    "    Dataset Download Link - https://data.mendeley.com/datasets/r43wkvdk4w/1\n",
    "'''\n",
    "\n",
    "class  OCRDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, img_dir, targets):\n",
    "        self.img_dir = img_dir\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_dir)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        image = Image.open(self.img_dir[item])\n",
    "        image = image.resize((128, 64), resample=Image.BILINEAR)\n",
    "\n",
    "        targets = self.targets[item]\n",
    "\n",
    "        image = np.array(image)\n",
    "        image = np.stack((image,)*1, axis=-1)\n",
    "\n",
    "        # Reshape to tensor format supported by Pytorch (C, H, W)\n",
    "        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
    "\n",
    "        return {\n",
    "            \"images\": torch.tensor(image, dtype=torch.float),\n",
    "            \"targets\": torch.tensor(targets, dtype=torch.long),\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW MODEL\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_hidden_size = hidden_size // 2\n",
    "        \n",
    "        self.query = nn.Linear(hidden_size, self.attention_hidden_size)\n",
    "        self.key = nn.Linear(hidden_size, self.attention_hidden_size)\n",
    "        self.value = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.attention_hidden_size]))\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        Q = self.query(query)\n",
    "        K = self.key(key)\n",
    "        V = self.value(value)\n",
    "        \n",
    "        # Q: [batch_size, query_len, attention_hidden_size]\n",
    "        # K: [batch_size, key_len, attention_hidden_size]\n",
    "        # V: [batch_size, value_len, hidden_size]\n",
    "        \n",
    "        energy = torch.matmul(Q, K.permute(0, 2, 1)) / self.scale.to(query.device)\n",
    "        \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "            \n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "        \n",
    "        output = torch.matmul(attention, V)\n",
    "        \n",
    "        return output, attention\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].detach()\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=kernel_size//2)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, 1, padding=kernel_size//2)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        \n",
    "        out += self.shortcut(residual)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class SpatialTransformer(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SpatialTransformer, self).__init__()\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=5, padding=2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 128, kernel_size=5, padding=2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(128 * 16 * 4, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 6)\n",
    "        )\n",
    "        \n",
    "        # Initialize transformation parameters\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 128 * 16 * 4)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "        \n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class FeaturePyramidNetwork(nn.Module):\n",
    "    def __init__(self, in_channels_list, out_channels):\n",
    "        super(FeaturePyramidNetwork, self).__init__()\n",
    "        self.inner_blocks = nn.ModuleList()\n",
    "        self.layer_blocks = nn.ModuleList()\n",
    "        \n",
    "        for in_channels in in_channels_list:\n",
    "            self.inner_blocks.append(nn.Conv2d(in_channels, out_channels, 1))\n",
    "            self.layer_blocks.append(nn.Conv2d(out_channels, out_channels, 3, padding=1))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        results = []\n",
    "        \n",
    "        for i, (inner_block, layer_block) in enumerate(zip(self.inner_blocks, self.layer_blocks)):\n",
    "            if i == 0:\n",
    "                out = inner_block(x)\n",
    "            else:\n",
    "                out = inner_block(x) + F.interpolate(results[-1], size=x.shape[-2:], mode='nearest')\n",
    "            out = layer_block(out)\n",
    "            results.append(out)\n",
    "            \n",
    "        return results\n",
    "\n",
    "class ComplexOCRModel(nn.Module):\n",
    "    def __init__(self, num_chars, input_channels=1, hidden_size=1024):\n",
    "        super(ComplexOCRModel, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = 512\n",
    "        \n",
    "        # Spatial Transformer for input preprocessing\n",
    "        self.stn = SpatialTransformer(input_channels)\n",
    "        \n",
    "        # Convolutional Backbone - Much deeper and wider\n",
    "        self.conv_backbone = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            \n",
    "            # Residual blocks\n",
    "            ResidualBlock(64, 128),\n",
    "            ResidualBlock(128, 128),\n",
    "            ResidualBlock(128, 256, stride=2),\n",
    "            ResidualBlock(256, 256),\n",
    "            ResidualBlock(256, 512, stride=2),\n",
    "            ResidualBlock(512, 512),\n",
    "            ResidualBlock(512, 1024, stride=2),\n",
    "            ResidualBlock(1024, 1024),\n",
    "        )\n",
    "        \n",
    "        # Feature Pyramid Network for multi-scale feature extraction\n",
    "        self.fpn = FeaturePyramidNetwork([256, 512, 1024], 256)\n",
    "        \n",
    "        # Sequence encoder\n",
    "        self.sequence_encoder = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 512, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(512),\n",
    "        )\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.positional_encoding = PositionalEncoding(hidden_size)\n",
    "        \n",
    "        # Bidirectional GRU layers\n",
    "        self.gru_layers = nn.ModuleList([\n",
    "            nn.GRU(512, hidden_size//2, bidirectional=True, batch_first=True),\n",
    "            nn.GRU(hidden_size, hidden_size//2, bidirectional=True, batch_first=True),\n",
    "            nn.GRU(hidden_size, hidden_size//2, bidirectional=True, batch_first=True)\n",
    "        ])\n",
    "        \n",
    "        # Self-attention mechanism\n",
    "        self.self_attention = nn.ModuleList([\n",
    "            Attention(hidden_size) for _ in range(4)\n",
    "        ])\n",
    "        \n",
    "        # Fully connected layers for feature transformation\n",
    "        self.fc_layers = nn.ModuleList([\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "        ])\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # Character prediction head\n",
    "        self.char_pred = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, num_chars + 1)  # +1 for blank in CTC\n",
    "        )\n",
    "        \n",
    "        # Additional hidden layers to increase model size\n",
    "        self.additional_layers = nn.ModuleList([\n",
    "            nn.Linear(hidden_size, hidden_size) for _ in range(30)\n",
    "        ])\n",
    "        \n",
    "        # Large parameter tensors to increase model size\n",
    "        self.large_param1 = nn.Parameter(torch.randn(2000, 2000))\n",
    "        self.large_param2 = nn.Parameter(torch.randn(2000, 2000))\n",
    "        self.large_param3 = nn.Parameter(torch.randn(2000, 2000))\n",
    "        self.large_param4 = nn.Parameter(torch.randn(2000, 2000))\n",
    "        self.large_param5 = nn.Parameter(torch.randn(2000, 2000))\n",
    "        \n",
    "    def forward(self, images, targets=None):\n",
    "        bs, c, h, w = images.size()\n",
    "        \n",
    "        # Apply spatial transformer\n",
    "        x = self.stn(images)\n",
    "        \n",
    "        # Extract features through convolutional backbone\n",
    "        x = self.conv_backbone(x)\n",
    "        \n",
    "        # Apply FPN to get multi-scale features - use the last level for sequence modeling\n",
    "        fpn_features = self.fpn(x)\n",
    "        x = fpn_features[-1]\n",
    "        \n",
    "        # Prepare for sequence modeling\n",
    "        x = self.sequence_encoder(x)\n",
    "        \n",
    "        # Collapse height dimension for sequence modeling\n",
    "        x = x.mean(dim=2)  # Average pooling over height\n",
    "        x = x.permute(0, 2, 1)  # [bs, seq_len, channels]\n",
    "        \n",
    "        # Apply positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        # Apply GRU layers with residual connections\n",
    "        gru_out = x\n",
    "        for i, gru in enumerate(self.gru_layers):\n",
    "            residual = gru_out\n",
    "            gru_out, _ = gru(gru_out)\n",
    "            gru_out = gru_out + residual\n",
    "            \n",
    "            # Apply self-attention after each GRU layer\n",
    "            if i < len(self.self_attention):\n",
    "                attn_out, _ = self.self_attention[i](gru_out, gru_out, gru_out)\n",
    "                gru_out = gru_out + attn_out\n",
    "                \n",
    "            # Apply fully connected transformation\n",
    "            if i < len(self.fc_layers):\n",
    "                fc_out = self.fc_layers[i](gru_out)\n",
    "                gru_out = gru_out + fc_out\n",
    "                \n",
    "            gru_out = self.dropout(gru_out)\n",
    "        \n",
    "        # Apply character prediction head\n",
    "        x = self.char_pred(gru_out)\n",
    "        \n",
    "        # Prepare for CTC loss\n",
    "        x = x.permute(1, 0, 2)  # [seq_len, bs, num_classes]\n",
    "        \n",
    "        if targets is not None:\n",
    "            log_probs = F.log_softmax(x, 2)\n",
    "            input_lengths = torch.full(\n",
    "                size=(bs,), fill_value=log_probs.size(0), dtype=torch.int32\n",
    "            )\n",
    "            \n",
    "            # Calculate actual lengths of targets (excluding padding zeros)\n",
    "            target_lengths = []\n",
    "            for i in range(bs):\n",
    "                count = 0\n",
    "                for j in range(targets.size(1)):\n",
    "                    if targets[i, j] != 0:\n",
    "                        count += 1\n",
    "                target_lengths.append(max(1, count))  # Ensure at least length 1\n",
    "            \n",
    "            target_lengths = torch.tensor(target_lengths, dtype=torch.int32)\n",
    "            \n",
    "            # Use CTC loss\n",
    "            loss = nn.CTCLoss(blank=0, reduction='mean')(\n",
    "                log_probs, targets, input_lengths, target_lengths\n",
    "            )\n",
    "            return x, loss\n",
    "        \n",
    "        return x, None\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the model OLD MODEL\n",
    "\n",
    "class OCRModel(nn.Module):\n",
    "    def __init__(self, num_chars):\n",
    "        super(OCRModel, self).__init__()\n",
    "        self.conv_1 = nn.Conv2d(1, 128, kernel_size=(3, 6), padding=(1, 1))\n",
    "        self.pool_1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.conv_2 = nn.Conv2d(128, 64, kernel_size=(3, 6), padding=(1, 1))\n",
    "        self.pool_2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        self.linear_1 = nn.Linear(1024, 64) # 1024 = 64*16\n",
    "        self.drop_1 = nn.Dropout(0.2)\n",
    "        self.gru = nn.GRU(64, 32, bidirectional=True, num_layers=2, dropout=0.25, batch_first=True)\n",
    "        self.output = nn.Linear(64, num_chars + 1)\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        bs, c, h, w = images.size()\n",
    "        # print(\"bs, c, h, w = \", bs, c, h, w)\n",
    "        x = F.relu(self.conv_1(images))\n",
    "        # print(x.size())\n",
    "        x = self.pool_1(x)\n",
    "        # print(x.size())\n",
    "        x = F.relu(self.conv_2(x))\n",
    "        # print(x.size())\n",
    "        x = self.pool_2(x) # [8, 64, 16, 29] (bs, c, h, w)\n",
    "        # print(x.size())\n",
    "        \n",
    "        x = x.permute(0, 3, 1, 2) # bs, w, c, h\n",
    "        # print(x.size())           # 8, 29, 64, 16 \n",
    "        x = x.view(bs, x.size(1), -1)\n",
    "        # print(x.size())\n",
    "        x = F.relu(self.linear_1(x))\n",
    "        x = self.drop_1(x)\n",
    "        # print(x.size())\n",
    "        \n",
    "        x, _ = self.gru(x)\n",
    "        # print(x.size())\n",
    "        x = self.output(x)\n",
    "        # print(x.size())\n",
    "        \n",
    "        x = x.permute(1, 0, 2)\n",
    "\n",
    "        if targets is not None:\n",
    "            log_probs = F.log_softmax(x, 2).to(torch.float64)\n",
    "            input_lengths = torch.full(\n",
    "                size=(bs,), fill_value=log_probs.size(0), dtype=torch.int32\n",
    "            )\n",
    "            # print(input_lengths)\n",
    "            target_lengths = torch.full(\n",
    "                size=(bs,), fill_value=targets.size(1), dtype=torch.int32\n",
    "            )\n",
    "            # print(target_lengths)\n",
    "            loss = nn.CTCLoss(blank=0)(\n",
    "                log_probs, targets, input_lengths, target_lengths\n",
    "            )\n",
    "#             print(loss)\n",
    "            return x, loss\n",
    "\n",
    "        return x, None\n",
    "\n",
    "\n",
    "#\n",
    "if __name__ == \"__main__\":\n",
    "    cm = OCRModel(115)\n",
    "    img = torch.rand((32, 1, 64, 128))\n",
    "    x, _ = cm(img, torch.rand((32, 15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(x):\n",
    "    if len(x) < 2:\n",
    "        return x\n",
    "    fin = \"\"\n",
    "    for j in x:\n",
    "        if fin == \"\":\n",
    "            fin = j\n",
    "        else:\n",
    "            if j == fin[-1]:\n",
    "                continue\n",
    "            else:\n",
    "                fin = fin + j\n",
    "    return fin\n",
    "\n",
    "\n",
    "def decode_predictions(preds, encoder):\n",
    "    preds = preds.permute(1, 0, 2)\n",
    "    preds = torch.softmax(preds, 2)\n",
    "    preds = torch.argmax(preds, 2)\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    word_preds = []\n",
    "    for j in range(preds.shape[0]):\n",
    "        temp = []\n",
    "        for k in preds[j, :]:\n",
    "            k = k - 1\n",
    "            if k == -1:\n",
    "                temp.append(\"°\")\n",
    "            else:\n",
    "                p = encoder.inverse_transform([k])[0]\n",
    "                temp.append(p)\n",
    "        tp = \"\".join(temp)\n",
    "        word_preds.append(remove_duplicates(tp))\n",
    "    return word_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train and test functions\n",
    "\n",
    "def train_fn(model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    fin_loss = 0\n",
    "    tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "    \n",
    "\n",
    "    for data in tk0:\n",
    "        for key, value in data.items():\n",
    "            data[key] = value.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        optimizer.zero_grad()\n",
    "        _, loss = model(**data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        fin_loss += loss.item()\n",
    "    return fin_loss / len(data_loader)\n",
    "\n",
    "\n",
    "def eval_fn(model, data_loader):\n",
    "    model.eval()\n",
    "    fin_loss = 0\n",
    "    fin_preds = []\n",
    "    with torch.no_grad():\n",
    "        tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "        for data in tk0:\n",
    "            for key, value in data.items():\n",
    "                data[key] = value.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            batch_preds, loss = model(**data)\n",
    "            fin_loss += loss.item()\n",
    "            fin_preds.append(batch_preds)\n",
    "        return fin_preds, fin_loss / len(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = './img/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 226/392 [00:06<00:04, 33.60it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 89\u001b[39m\n\u001b[32m     84\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m     85\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Train Loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Test Loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Accuracy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     86\u001b[39m         )\n\u001b[32m     87\u001b[39m         scheduler.step(test_loss)\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     70\u001b[39m num_epoch = \u001b[32m10\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epoch):\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     train_loss = \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m     valid_preds, test_loss = eval_fn(model, test_loader)\n\u001b[32m     74\u001b[39m     valid_word_preds = []\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mtrain_fn\u001b[39m\u001b[34m(model, data_loader, optimizer)\u001b[39m\n\u001b[32m     14\u001b[39m     loss.backward()\n\u001b[32m     15\u001b[39m     optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     fin_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m fin_loss / \u001b[38;5;28mlen\u001b[39m(data_loader)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    print('train function is running')\n",
    "    image_files = glob.glob(os.path.join(filepath, '*jpg'))\n",
    "    targets_orig = [x.split(\"/\")[1].split(\" \")[0] for x in image_files]\n",
    "#     print(targets_orig)\n",
    "    targets = [[c for c in x] for x in targets_orig]\n",
    "    targets_flat = [c for clist in targets for c in clist]\n",
    "    \n",
    "    lbl_enc = preprocessing.LabelEncoder()\n",
    "    lbl_enc.fit(targets_flat)\n",
    "    targets_enc = [lbl_enc.transform(x) for x in targets]\n",
    "    targets_enc = np.array(targets_enc) + 1\n",
    "#     print(targets_enc)\n",
    "    \n",
    "    #############################################################################################\n",
    "#     num = 3635\n",
    "#     print(targets[num])  # target length (# 12650 = 9)\n",
    "#     print(\"Target label length =\", len(targets_enc[num]))\n",
    "    #############################################################################################\n",
    "    \n",
    "    \n",
    "    # add padding to labels to make the target length equal for every target/label\n",
    "    maxlen = len(max(targets, key=len)) # to get the length of the largest label\n",
    "    # print(maxlen)\n",
    "    # print(max(targets, key=len))\n",
    "    \n",
    "    # iterating over every target and adding 0 at the last\n",
    "    for item in range(len(targets_enc)):\n",
    "        difference = maxlen - len(targets_enc[item]) \n",
    "        for i in range(difference):\n",
    "            targets_enc[item] = np.append(targets_enc[item], 0)\n",
    "#             np.pad(targets_enc[item], (0, difference), 'constant')\n",
    "\n",
    "    \n",
    "    print(\"Total unique classes/characters:\", len(lbl_enc.classes_))\n",
    "#     print(lbl_enc.classes_[114])\n",
    "#     print(np.unique(targets_flat))\n",
    "    \n",
    "    # divide into train test \n",
    "    (\n",
    "        train_imgs,\n",
    "        test_imgs,\n",
    "        train_targets,\n",
    "        test_targets,\n",
    "        train_orig_targets,\n",
    "        test_orig_targets,\n",
    "    ) = model_selection.train_test_split (\n",
    "        image_files, targets_enc, targets_orig, test_size = 0.2, random_state = 42\n",
    "    )\n",
    "    \n",
    "    # loading images and their corresponding labels to train and test dataset\n",
    "    train_dataset = OCRDataset(img_dir = train_imgs, targets = train_targets)\n",
    "    test_dataset = OCRDataset(img_dir = test_imgs, targets = test_targets)\n",
    "    \n",
    "    # defining the data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    \n",
    "    # model goes here\n",
    "    model = OCRModel(len(lbl_enc.classes_))\n",
    "    model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, factor=0.8, patience=5, verbose=True\n",
    "    )\n",
    "\n",
    "    # define number of epoch and start training\n",
    "    num_epoch = 10\n",
    "    for epoch in range(num_epoch):\n",
    "        train_loss = train_fn(model, train_loader, optimizer)\n",
    "        valid_preds, test_loss = eval_fn(model, test_loader)\n",
    "        valid_word_preds = []\n",
    "        \n",
    "        for vp in valid_preds:\n",
    "            current_preds = decode_predictions(vp, lbl_enc)\n",
    "            valid_word_preds.extend(current_preds)\n",
    "        combined = list(zip(test_orig_targets, valid_word_preds))\n",
    "        print(combined[:10])\n",
    "        test_dup_rem = [remove_duplicates(c) for c in test_orig_targets]\n",
    "        accuracy = metrics.accuracy_score(test_dup_rem, valid_word_preds)\n",
    "        pprint.pprint(list(zip(test_orig_targets, valid_word_preds))[6:11])\n",
    "        print(\n",
    "            f\"Epoch={epoch}, Train Loss={train_loss}, Test Loss={test_loss} Accuracy={accuracy}\"\n",
    "        )\n",
    "        scheduler.step(test_loss)\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize train data and its shape\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# %matplotlib inline\n",
    "\n",
    "# npimg = train_dataset[200]['images'].np()\n",
    "# print(npimg.shape) # print current shape (torch style)\n",
    "\n",
    "# # change the orientation of the image to display\n",
    "# npimg = np.transpose(npimg, (1, 2, 0)).astype(np.float32)\n",
    "# print(npimg.shape)\n",
    "\n",
    "# plt.imshow(npimg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
