{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import pprint\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    OCR DATASET CLASS\n",
    "    Dataset Used = BanglaWriting\n",
    "    Dataset Manual = https://arxiv.org/pdf/2011.07499.pdf\n",
    "    Dataset Download Link - https://data.mendeley.com/datasets/r43wkvdk4w/1\n",
    "'''\n",
    "\n",
    "class  OCRDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, img_dir, targets):\n",
    "        self.img_dir = img_dir\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_dir)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        image = Image.open(self.img_dir[item])\n",
    "        image = image.resize((128, 64), resample=Image.BILINEAR)\n",
    "\n",
    "        targets = self.targets[item]\n",
    "\n",
    "        image = np.array(image)\n",
    "        image = np.stack((image,)*1, axis=-1)\n",
    "\n",
    "        # Reshape to tensor format supported by Pytorch (C, H, W)\n",
    "        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
    "\n",
    "        return {\n",
    "            \"images\": torch.tensor(image, dtype=torch.float),\n",
    "            \"targets\": torch.tensor(targets, dtype=torch.long),\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the model\n",
    "\n",
    "class OCRModel(nn.Module):\n",
    "    def __init__(self, num_chars):\n",
    "        super(OCRModel, self).__init__()\n",
    "        self.conv_1 = nn.Conv2d(1, 128, kernel_size=(3, 6), padding=(1, 1))\n",
    "        self.pool_1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.conv_2 = nn.Conv2d(128, 64, kernel_size=(3, 6), padding=(1, 1))\n",
    "        self.pool_2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        self.linear_1 = nn.Linear(1024, 64) # 1024 = 64*16\n",
    "        self.drop_1 = nn.Dropout(0.2)\n",
    "        self.gru = nn.GRU(64, 32, bidirectional=True, num_layers=2, dropout=0.25, batch_first=True)\n",
    "        self.output = nn.Linear(64, num_chars + 1)\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        bs, c, h, w = images.size()\n",
    "        # print(\"bs, c, h, w = \", bs, c, h, w)\n",
    "        x = F.relu(self.conv_1(images))\n",
    "        # print(x.size())\n",
    "        x = self.pool_1(x)\n",
    "        # print(x.size())\n",
    "        x = F.relu(self.conv_2(x))\n",
    "        # print(x.size())\n",
    "        x = self.pool_2(x) # [8, 64, 16, 29] (bs, c, h, w)\n",
    "        # print(x.size())\n",
    "        \n",
    "        x = x.permute(0, 3, 1, 2) # bs, w, c, h\n",
    "        # print(x.size())           # 8, 29, 64, 16 \n",
    "        x = x.view(bs, x.size(1), -1)\n",
    "        # print(x.size())\n",
    "        x = F.relu(self.linear_1(x))\n",
    "        x = self.drop_1(x)\n",
    "        # print(x.size())\n",
    "        \n",
    "        x, _ = self.gru(x)\n",
    "        # print(x.size())\n",
    "        x = self.output(x)\n",
    "        # print(x.size())\n",
    "        \n",
    "        x = x.permute(1, 0, 2)\n",
    "\n",
    "        if targets is not None:\n",
    "            log_probs = F.log_softmax(x, 2).to(torch.float64)\n",
    "            input_lengths = torch.full(\n",
    "                size=(bs,), fill_value=log_probs.size(0), dtype=torch.int32\n",
    "            )\n",
    "            # print(input_lengths)\n",
    "            target_lengths = torch.full(\n",
    "                size=(bs,), fill_value=targets.size(1), dtype=torch.int32\n",
    "            )\n",
    "            # print(target_lengths)\n",
    "            loss = nn.CTCLoss(blank=0)(\n",
    "                log_probs, targets, input_lengths, target_lengths\n",
    "            )\n",
    "#             print(loss)\n",
    "            return x, loss\n",
    "\n",
    "        return x, None\n",
    "\n",
    "\n",
    "#\n",
    "if __name__ == \"__main__\":\n",
    "    cm = OCRModel(115)\n",
    "    img = torch.rand((32, 1, 64, 128))\n",
    "    x, _ = cm(img, torch.rand((32, 15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(x):\n",
    "    if len(x) < 2:\n",
    "        return x\n",
    "    fin = \"\"\n",
    "    for j in x:\n",
    "        if fin == \"\":\n",
    "            fin = j\n",
    "        else:\n",
    "            if j == fin[-1]:\n",
    "                continue\n",
    "            else:\n",
    "                fin = fin + j\n",
    "    return fin\n",
    "\n",
    "\n",
    "def decode_predictions(preds, encoder):\n",
    "    preds = preds.permute(1, 0, 2)\n",
    "    preds = torch.softmax(preds, 2)\n",
    "    preds = torch.argmax(preds, 2)\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    word_preds = []\n",
    "    for j in range(preds.shape[0]):\n",
    "        temp = []\n",
    "        for k in preds[j, :]:\n",
    "            k = k - 1\n",
    "            if k == -1:\n",
    "                temp.append(\"Â°\")\n",
    "            else:\n",
    "                p = encoder.inverse_transform([k])[0]\n",
    "                temp.append(p)\n",
    "        tp = \"\".join(temp)\n",
    "        word_preds.append(remove_duplicates(tp))\n",
    "    return word_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train and test functions\n",
    "\n",
    "def train_fn(model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    fin_loss = 0\n",
    "    tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "    \n",
    "\n",
    "    for data in tk0:\n",
    "        for key, value in data.items():\n",
    "            data[key] = value.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        optimizer.zero_grad()\n",
    "        _, loss = model(**data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        fin_loss += loss.item()\n",
    "    return fin_loss / len(data_loader)\n",
    "\n",
    "\n",
    "def eval_fn(model, data_loader):\n",
    "    model.eval()\n",
    "    fin_loss = 0\n",
    "    fin_preds = []\n",
    "    with torch.no_grad():\n",
    "        tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "        for data in tk0:\n",
    "            for key, value in data.items():\n",
    "                data[key] = value.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            batch_preds, loss = model(**data)\n",
    "            fin_loss += loss.item()\n",
    "            fin_preds.append(batch_preds)\n",
    "        return fin_preds, fin_loss / len(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = './img' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train function is running\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'np' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 105\u001b[39m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28mprint\u001b[39m(npimg.shape)\n\u001b[32m    103\u001b[39m     plt.imshow(npimg)\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     10\u001b[39m     lbl_enc.fit(targets_flat)\n\u001b[32m     11\u001b[39m     targets_enc = [lbl_enc.transform(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     targets_enc = \u001b[43mnp\u001b[49m.array(targets_enc) + \u001b[32m1\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m#     print(targets_enc)\u001b[39;00m\n\u001b[32m     14\u001b[39m \n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m#############################################################################################\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m \n\u001b[32m     22\u001b[39m     \u001b[38;5;66;03m# add padding to labels to make the target length equal for every target/label\u001b[39;00m\n\u001b[32m     23\u001b[39m     maxlen = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mmax\u001b[39m(targets, key=\u001b[38;5;28mlen\u001b[39m)) \u001b[38;5;66;03m# to get the length of the largest label\u001b[39;00m\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'np' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    print('train function is running')\n",
    "    image_files = glob.glob(os.path.join(filepath, '*jpg'))\n",
    "    targets_orig = [x.split(\"/\")[1].split(\" \")[0] for x in image_files]\n",
    "#     print(targets_orig)\n",
    "    targets = [[c for c in x] for x in targets_orig]\n",
    "    targets_flat = [c for clist in targets for c in clist]\n",
    "    \n",
    "    lbl_enc = preprocessing.LabelEncoder()\n",
    "    lbl_enc.fit(targets_flat)\n",
    "    targets_enc = [lbl_enc.transform(x) for x in targets]\n",
    "    targets_enc = np.array(targets_enc) + 1\n",
    "#     print(targets_enc)\n",
    "    \n",
    "    #############################################################################################\n",
    "#     num = 3635\n",
    "#     print(targets[num])  # target length (# 12650 = 9)\n",
    "#     print(\"Target label length =\", len(targets_enc[num]))\n",
    "    #############################################################################################\n",
    "    \n",
    "    \n",
    "    # add padding to labels to make the target length equal for every target/label\n",
    "    maxlen = len(max(targets, key=len)) # to get the length of the largest label\n",
    "    # print(maxlen)\n",
    "    # print(max(targets, key=len))\n",
    "    \n",
    "    # iterating over every target and adding 0 at the last\n",
    "    for item in range(len(targets_enc)):\n",
    "        difference = maxlen - len(targets_enc[item]) \n",
    "        for i in range(difference):\n",
    "            targets_enc[item] = np.append(targets_enc[item], 0)\n",
    "#             np.pad(targets_enc[item], (0, difference), 'constant')\n",
    "\n",
    "    \n",
    "    print(\"Total unique classes/characters:\", len(lbl_enc.classes_))\n",
    "#     print(lbl_enc.classes_[114])\n",
    "#     print(np.unique(targets_flat))\n",
    "    \n",
    "    # divide into train test \n",
    "    (\n",
    "        train_imgs,\n",
    "        test_imgs,\n",
    "        train_targets,\n",
    "        test_targets,\n",
    "        train_orig_targets,\n",
    "        test_orig_targets,\n",
    "    ) = model_selection.train_test_split (\n",
    "        image_files, targets_enc, targets_orig, test_size = 0.2, random_state = 42\n",
    "    )\n",
    "    \n",
    "    # loading images and their corresponding labels to train and test dataset\n",
    "    train_dataset = OCRDataset(img_dir = train_imgs, targets = train_targets)\n",
    "    test_dataset = OCRDataset(img_dir = test_imgs, targets = test_targets)\n",
    "    \n",
    "    # defining the data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    \n",
    "    # model goes here\n",
    "    model = OCRModel(len(lbl_enc.classes_))\n",
    "    model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, factor=0.8, patience=5, verbose=True\n",
    "    )\n",
    "\n",
    "    # define number of epoch and start training\n",
    "    num_epoch = 10\n",
    "    for epoch in range(num_epoch):\n",
    "        train_loss = train_fn(model, train_loader, optimizer)\n",
    "        valid_preds, test_loss = eval_fn(model, test_loader)\n",
    "        valid_word_preds = []\n",
    "        \n",
    "        for vp in valid_preds:\n",
    "            current_preds = decode_predictions(vp, lbl_enc)\n",
    "            valid_word_preds.extend(current_preds)\n",
    "        combined = list(zip(test_orig_targets, valid_word_preds))\n",
    "        print(combined[:10])\n",
    "        test_dup_rem = [remove_duplicates(c) for c in test_orig_targets]\n",
    "        accuracy = metrics.accuracy_score(test_dup_rem, valid_word_preds)\n",
    "        pprint.pprint(list(zip(test_orig_targets, valid_word_preds))[6:11])\n",
    "        print(\n",
    "            f\"Epoch={epoch}, Train Loss={train_loss}, Test Loss={test_loss} Accuracy={accuracy}\"\n",
    "        )\n",
    "        scheduler.step(test_loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    %matplotlib inline\n",
    "\n",
    "    npimg = train_dataset[200]['images'].numpy()\n",
    "    print(npimg.shape) # print current shape (torch style)\n",
    "\n",
    "    # change the orientation of the image to display\n",
    "    npimg = np.transpose(npimg, (1, 2, 0)).astype(np.float32)\n",
    "    print(npimg.shape)\n",
    "\n",
    "    plt.imshow(npimg)\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mmatplotlib\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minline\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m npimg = \u001b[43mtrain_dataset\u001b[49m[\u001b[32m200\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mimages\u001b[39m\u001b[33m'\u001b[39m].numpy()\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(npimg.shape) \u001b[38;5;66;03m# print current shape (torch style)\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# change the orientation of the image to display\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Visualize train data and its shape\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# %matplotlib inline\n",
    "\n",
    "# npimg = train_dataset[200]['images'].np()\n",
    "# print(npimg.shape) # print current shape (torch style)\n",
    "\n",
    "# # change the orientation of the image to display\n",
    "# npimg = np.transpose(npimg, (1, 2, 0)).astype(np.float32)\n",
    "# print(npimg.shape)\n",
    "\n",
    "# plt.imshow(npimg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
